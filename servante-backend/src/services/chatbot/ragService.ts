import { chromaService } from './chromaService';

// ============================================
// TYPES ET INTERFACES
// ============================================

interface SearchResult {
  id: string;
  content: string;
  metadata: {
    title: string;
    filename: string;
    category?: string;
    chunkIndex?: number;
    totalChunks?: number;
    mimetype?: string;
    uploadedAt?: string;
    size?: number;
    tags?: string[];
    [key: string]: any;
  };
  distance?: number;
}

interface OllamaResponse {
  model: string;
  created_at: string;
  response: string;
  done: boolean;
  context?: number[];
  total_duration?: number;
  load_duration?: number;
  prompt_eval_count?: number;
  prompt_eval_duration?: number;
  eval_count?: number;
  eval_duration?: number;
}

interface OllamaHealthStatus {
  available: boolean;
  models: string[];
  error?: string;
}

interface GenerateAnswerResult {
  success: boolean;
  answer: string;
  sources: Array<{
    title: string;
    filename: string;
    category?: string;
    chunkIndex?: number;
    relevance: number;
  }>;
  metadata: {
    query: string;
    chunksUsed: number;
    model: string;
    processingTime: number;
  };
  error?: string;
}

// ============================================
// CONFIGURATION
// ============================================

const OLLAMA_CONFIG = {
  baseUrl: process.env.OLLAMA_URL || 'http://localhost:11434',
  model: process.env.OLLAMA_MODEL || 'llama3.2:latest',
  timeout: 60000,
};

const RAG_CONFIG = {
  topK: 5,
  maxContextLength: 4000,
  temperature: 0.7,
  systemPrompt: `Tu es un assistant virtuel expert qui aide les utilisateurs √† trouver des informations dans la base de connaissances.

INSTRUCTIONS IMPORTANTES:
1. R√©ponds UNIQUEMENT en te basant sur les informations fournies dans le CONTEXTE ci-dessous
2. Si l'information n'est pas dans le contexte, dis clairement "Je n'ai pas cette information dans ma base de connaissances"
3. Sois pr√©cis, concis et professionnel
4. Cite les sources quand c'est pertinent (ex: "Selon le guide utilisateur...")
5. Structure ta r√©ponse avec des listes √† puces si n√©cessaire
6. R√©ponds en fran√ßais`,
};

// ============================================
// FONCTIONS UTILITAIRES
// ============================================

/**
 * V√©rifie la sant√© du service Ollama
 */
export async function checkOllamaHealth(): Promise<OllamaHealthStatus> {
  try {
    console.log('üîç V√©rification de la sant√© d\'Ollama...');
    
    const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/tags`, {
      method: 'GET',
      signal: AbortSignal.timeout(5000),
    });

    if (!response.ok) {
      throw new Error(`Ollama API retourn√© le statut ${response.status}`);
    }

    const data = await response.json();
    const models = data.models?.map((m: any) => m.name) || [];

    console.log(`‚úÖ Ollama disponible avec ${models.length} mod√®le(s):`, models);

    return {
      available: true,
      models,
    };
  } catch (error) {
    console.error('‚ùå Ollama non disponible:', error);
    return {
      available: false,
      models: [],
      error: error instanceof Error ? error.message : 'Erreur inconnue',
    };
  }
}

/**
 * Construit le prompt enrichi avec le contexte RAG
 */
function buildPrompt(query: string, contexts: SearchResult[]): string {
  const contextText = contexts
    .map((ctx: SearchResult, idx: number) => {
      const source = `[Source ${idx + 1}: ${ctx.metadata.title || ctx.metadata.filename}]`;
      return `${source}\n${ctx.content}\n`;
    })
    .join('\n---\n\n');

  return `${RAG_CONFIG.systemPrompt}

CONTEXTE:
${contextText}

QUESTION DE L'UTILISATEUR:
${query}

R√âPONSE:`;
}

/**
 * Appelle Ollama pour g√©n√©rer une r√©ponse
 */
async function callOllama(prompt: string): Promise<string> {
  console.log('ü§ñ Appel √† Ollama pour g√©n√©ration...');
  
  const requestBody = {
    model: OLLAMA_CONFIG.model,
    prompt: prompt,
    stream: false,
    options: {
      temperature: RAG_CONFIG.temperature,
      num_predict: 1000,
    },
  };

  console.log(`üì§ Mod√®le: ${OLLAMA_CONFIG.model}`);
  console.log(`üì§ Prompt length: ${prompt.length} caract√®res`);

  const response = await fetch(`${OLLAMA_CONFIG.baseUrl}/api/generate`, {
    method: 'POST',
    headers: {
      'Content-Type': 'application/json',
    },
    body: JSON.stringify(requestBody),
    signal: AbortSignal.timeout(OLLAMA_CONFIG.timeout),
  });

  if (!response.ok) {
    const errorText = await response.text();
    throw new Error(`Ollama API erreur (${response.status}): ${errorText}`);
  }

  const data: OllamaResponse = await response.json();
  
  console.log('‚úÖ R√©ponse g√©n√©r√©e par Ollama');
  console.log(`üìä Tokens g√©n√©r√©s: ${data.eval_count || 'N/A'}`);
  
  return data.response.trim();
}

/**
 * Calcule un score de pertinence (0-100)
 */
function calculateRelevance(distance?: number): number {
  if (distance === undefined) return 80;
  const relevance = Math.max(0, Math.min(100, (1 - distance / 2) * 100));
  return Math.round(relevance);
}

// ============================================
// FONCTION PRINCIPALE RAG
// ============================================

/**
 * G√©n√®re une r√©ponse √† partir d'une question en utilisant RAG
 */
export async function generateAnswer(
  query: string,
  topK: number = RAG_CONFIG.topK
): Promise<GenerateAnswerResult> {
  const startTime = Date.now();

  try {
    console.log(`\n${'='.repeat(60)}`);
    console.log(`üîç G√âN√âRATION DE R√âPONSE RAG`);
    console.log(`${'='.repeat(60)}`);
    console.log(`üìù Question: "${query}"`);
    console.log(`üî¢ Top-K: ${topK}`);

    // 1. V√©rifier qu'Ollama est disponible
    const health = await checkOllamaHealth();
    if (!health.available) {
      return {
        success: false,
        answer: 'Le service de g√©n√©ration de r√©ponses (Ollama) n\'est pas disponible. Veuillez v√©rifier qu\'Ollama est d√©marr√©.',
        sources: [],
        metadata: {
          query,
          chunksUsed: 0,
          model: OLLAMA_CONFIG.model,
          processingTime: Date.now() - startTime,
        },
        error: health.error,
      };
    }

    // 2. Recherche s√©mantique dans ChromaDB
    console.log(`\nüìö Recherche de chunks pertinents...`);
    const searchResults = await chromaService.searchDocuments(query, topK);

    if (!searchResults || searchResults.length === 0) {
      console.log('‚ö†Ô∏è Aucun chunk pertinent trouv√©');
      return {
        success: true,
        answer: 'Je n\'ai trouv√© aucune information pertinente dans ma base de connaissances pour r√©pondre √† votre question. Pourriez-vous reformuler ou pr√©ciser votre demande ?',
        sources: [],
        metadata: {
          query,
          chunksUsed: 0,
          model: OLLAMA_CONFIG.model,
          processingTime: Date.now() - startTime,
        },
      };
    }

    console.log(`‚úÖ ${searchResults.length} chunk(s) trouv√©(s)`);
    
    // Convertir les r√©sultats de votre chromaService vers SearchResult
    const typedResults: SearchResult[] = searchResults.map((result: any) => ({
      id: result.id,
      content: result.content,
      metadata: result.metadata,
      distance: result.distance,
    }));

    typedResults.forEach((result: SearchResult, idx: number) => {
      const relevance = calculateRelevance(result.distance);
      console.log(`   ${idx + 1}. ${result.metadata.title} (pertinence: ${relevance}%)`);
    });

    // 3. Construire le prompt avec contexte
    console.log(`\nüìù Construction du prompt RAG...`);
    const prompt = buildPrompt(query, typedResults);

    // 4. G√©n√©rer la r√©ponse avec Ollama
    const answer = await callOllama(prompt);

    // 5. Pr√©parer les sources
    const sources = typedResults.map((result: SearchResult) => ({
      title: result.metadata.title || 'Sans titre',
      filename: result.metadata.filename || 'Inconnu',
      category: result.metadata.category || 'general',
      chunkIndex: result.metadata.chunkIndex,
      relevance: calculateRelevance(result.distance),
    }));

    const processingTime = Date.now() - startTime;

    console.log(`\n‚úÖ R√©ponse g√©n√©r√©e avec succ√®s`);
    console.log(`‚è±Ô∏è  Temps de traitement: ${processingTime}ms`);
    console.log(`üìä Longueur r√©ponse: ${answer.length} caract√®res`);
    console.log(`${'='.repeat(60)}\n`);

    return {
      success: true,
      answer,
      sources,
      metadata: {
        query,
        chunksUsed: typedResults.length,
        model: OLLAMA_CONFIG.model,
        processingTime,
      },
    };
  } catch (error) {
    console.error('‚ùå Erreur lors de la g√©n√©ration:', error);
    
    return {
      success: false,
      answer: 'D√©sol√©, une erreur est survenue lors de la g√©n√©ration de la r√©ponse. Veuillez r√©essayer.',
      sources: [],
      metadata: {
        query,
        chunksUsed: 0,
        model: OLLAMA_CONFIG.model,
        processingTime: Date.now() - startTime,
      },
      error: error instanceof Error ? error.message : 'Erreur inconnue',
    };
  }
}

/**
 * G√©n√®re une r√©ponse en streaming
 */
export async function generateAnswerStream(
  query: string,
  topK: number = RAG_CONFIG.topK,
  onChunk: (chunk: string) => void
): Promise<GenerateAnswerResult> {
  return generateAnswer(query, topK);
}